{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "203451\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "stemmer = nltk.stem.porter.PorterStemmer()\n",
    "df = pd.read_csv('tweets.csv')\n",
    "df[0:10]\n",
    "print(len(df['text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-f027472e17d9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mhashtag_regex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr'#(?=\\w)'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m df_unique_stemmed.text = (df_unique_stemmed.text.str.replace(r,repl='')\n\u001b[0m\u001b[1;32m     20\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl_regex\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrepl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0musername_regex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrepl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/pandas/core/strings.py\u001b[0m in \u001b[0;36mreplace\u001b[0;34m(self, pat, repl, n, case, flags)\u001b[0m\n\u001b[1;32m   1572\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrepl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcase\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1573\u001b[0m         result = str_replace(self._data, pat, repl, n=n, case=case,\n\u001b[0;32m-> 1574\u001b[0;31m                              flags=flags)\n\u001b[0m\u001b[1;32m   1575\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wrap_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1576\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/pandas/core/strings.py\u001b[0m in \u001b[0;36mstr_replace\u001b[0;34m(arr, pat, repl, n, case, flags)\u001b[0m\n\u001b[1;32m    427\u001b[0m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrepl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 429\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_na_map\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    431\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/pandas/core/strings.py\u001b[0m in \u001b[0;36m_na_map\u001b[0;34m(f, arr, na_result, dtype)\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_na_map\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mna_result\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnan\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m     \u001b[0;31m# should really _check_ for NA\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_map\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mna_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mna_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mna_result\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/pandas/core/strings.py\u001b[0m in \u001b[0;36m_map\u001b[0;34m(f, arr, na_mask, na_value, dtype)\u001b[0m\n\u001b[1;32m    169\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m             \u001b[0mconvert\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 171\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_infer_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muint8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    172\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m             \u001b[0;31m# Reraise the exception if callable `f` got wrong number of args.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/src/inference.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer_mask\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/pandas/core/strings.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    423\u001b[0m         \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    424\u001b[0m         \u001b[0mregex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 425\u001b[0;31m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mregex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrepl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrepl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    426\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    427\u001b[0m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrepl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "df.dropna(subset=[\"text\"], inplace=True)\n",
    "\n",
    "retweetedRussianTweets = df.retweeted_status_id.isin(df.tweet_id) & df.retweeted_status_id.notnull()\n",
    "df2 = df.drop(df[retweetedRussianTweets].index)\n",
    "\n",
    "df_unique_stemmed = df2.drop_duplicates('text')\n",
    "\n",
    "r = re.compile(r'RT @\\w*:')\n",
    "retweet_series = df_unique_stemmed.text.str.contains(r)\n",
    "rt_field_series = df_unique_stemmed.retweeted_status_id.notnull()\n",
    "\n",
    "df_unique_stemmed[0:10]\n",
    "# df['text'] = df[\"text\"].apply(lambda x: [stemmer.stem(y) for y in x])\n",
    "url_regex = re.compile(r'https?:\\/\\/(www\\.)?[-a-zA-Z0-9@:%._\\+~#=]{1,256}\\.[a-z]{2,6}\\b([-a-zA-Z0-9@:%_\\+.~#?&//=]*)')\n",
    "# username_regex = re.compile(r'RT @\\w*')\n",
    "username_regex = re.compile(r'@(?=\\w)')\n",
    "hashtag_regex = re.compile(r'#(?=\\w)')\n",
    "\n",
    "df_unique_stemmed.text = (df_unique_stemmed.text.str.replace(r,repl='')\n",
    "    .str.replace(url_regex,repl='')\n",
    "    .str.replace(username_regex, repl='')\n",
    "    .str.replace(hashtag_regex, repl='')\n",
    "    .str.replace('\\n', repl='')\n",
    "    .str.replace('\\r', repl='')       \n",
    "#    .apply(lambda x: ' '.join([stemmer.stem(y) for y in x.split(' ')]))\n",
    "                         )\n",
    "\n",
    "df_unique_stemmed.dropna(subset=[\"text\"], inplace=True)\n",
    "df_unique_stemmed[~(~retweet_series & ~rt_field_series)]['text'].to_csv('real_stemmed.txt',index=False)\n",
    "df_unique_stemmed[~retweet_series & ~rt_field_series]['text'].to_csv('russian_stemmed.txt',index=False)\n",
    "\n",
    "\n",
    "print(len(df_unique_stemmed['text']))\n",
    "print(len(df_unique_stemmed[~(~retweet_series & ~rt_field_series)]['text']))\n",
    "print(len(df_unique_stemmed[~retweet_series & ~rt_field_series]['text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./tweets.csv')\n",
    "\n",
    "df.dropna(subset=[\"text\"], inplace=True)\n",
    "\n",
    "retweetedRussianTweets = df.retweeted_status_id.isin(df.tweet_id) & df.retweeted_status_id.notnull()\n",
    "df2 = df.drop(df[retweetedRussianTweets].index)\n",
    "\n",
    "df_unique = df2.drop_duplicates('text')\n",
    "\n",
    "r = re.compile(r'RT @\\w*:')\n",
    "retweet_series = df_unique.text.str.contains(r)\n",
    "rt_field_series = df_unique.retweeted_status_id.notnull()\n",
    "\n",
    "df_unique[0:10]\n",
    "# df['text'] = df[\"text\"].apply(lambda x: [stemmer.stem(y) for y in x])\n",
    "url_regex = re.compile(r'https?:\\/\\/(www\\.)?[-a-zA-Z0-9@:%._\\+~#=]{1,256}\\.[a-z]{2,6}\\b([-a-zA-Z0-9@:%_\\+.~#?&//=]*)')\n",
    "# username_regex = re.compile(r'RT @\\w*')\n",
    "username_regex = re.compile(r'@(?=\\w)')\n",
    "hashtag_regex = re.compile(r'#(?=\\w)')\n",
    "\n",
    "df_unique.text = (df_unique.text.str.replace(r,repl='')\n",
    "    .str.replace(url_regex,repl='')\n",
    "    .str.replace(username_regex, repl='')\n",
    "    .str.replace(hashtag_regex, repl='')\n",
    "    .str.replace('\\n', repl='')\n",
    "    .str.replace('\\r', repl=''))\n",
    "\n",
    "df_unique.dropna(subset=[\"text\"], inplace=True)\n",
    "\n",
    "df_unique[~(~retweet_series & ~rt_field_series)]['text'].to_csv('real.txt',index=False)\n",
    "df_unique[~retweet_series & ~rt_field_series]['text'].to_csv('russian.txt',index=False)\n",
    "# df_original[\"russian\"] = 0\n",
    "\n",
    "print(len(df_unique['text']))\n",
    "print(len(df_unique[~(~retweet_series & ~rt_field_series)]['text']))\n",
    "print(len(df_unique[~retweet_series & ~rt_field_series]['text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Add a column bool_russian which is our target column for the classifier\n",
    "df_unique['bool_russian'] = (~retweet_series & ~rt_field_series).apply(lambda x: 1 if x else 0)\n",
    "df_unique[:10][['text','bool_russian']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dfLength = len(df_unique['text'])\n",
    "\n",
    "## Here we will create out list of stopwords\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stoplist = stopwords.words('english')\n",
    "PS3StopWords = [\"ever\", \"one\", \"do\",\"does\",\"make\", \"go\", \"us\", \"to\", \"get\", \"about\", \"may\", \"s\", \".\", \",\", \"!\", \"i\", \"I\", '\\\"', \"?\", \";\", \"--\", \"--\", \"would\", \"could\", \"”\", \"Mr.\", \"Miss\", \"Mrs.\", \"don’t\", \"said\", \"can't\", \"didn't\", \"aren't\", \"I'm\", \"you're\", \"they're\", \"'s\"]\n",
    "stoplist.extend(PS3StopWords)\n",
    "personalAdditions = [\"it\", \"i'm\", \"|\", \"–\", \"-\", \"~\", \"you're\", \"thing\", '\"', \"…\", '…\"', \"\"]\n",
    "stoplist.extend(personalAdditions)\n",
    "\n",
    "from gensim import corpora\n",
    "tweets = [[word for word in tweet.lower().split() if word not in stoplist] for tweet in df_unique['text']]\n",
    "dictionary = corpora.Dictionary(tweets)\n",
    "\n",
    "## Here we will remove stop words and words that have a frequency less than 5\n",
    "\n",
    "from six import iteritems\n",
    "stop_ids = [dictionary.token2id[stopword] for stopword in stoplist\n",
    "           if stopword in dictionary.token2id]\n",
    "once_ids = [tokenid for tokenid, docfreq in iteritems(dictionary.dfs) if docfreq < 5]\n",
    "dictionary.filter_tokens(stop_ids + once_ids)  # remove stop words and words that appear only once\n",
    "dictionary.compactify()  # remove gaps in id sequence after words that were removed\n",
    "print(dictionary)\n",
    "print(tweets[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This following nested for loop creates the raw counts for the feature vectors\n",
    "\n",
    "print(len(df_unique['text'])) # Number of tweets\n",
    "print(len(dictionary)) # Length of the feature vectors\n",
    "\n",
    "featureVectors = np.zeros((len(df_unique['text']),len(dictionary)),int)\n",
    "\n",
    "for i in range(len(df_unique['text'])) :\n",
    "    for word in tweets[i]:\n",
    "        if word in dictionary.token2id :\n",
    "            word_id = dictionary.token2id[word]\n",
    "            featureVectors[i][word_id] += 1\n",
    "            \n",
    "print(featureVectors[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import numpy as np\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "npdata = featureVectors\n",
    "nptarget = np.array(df_unique['bool_russian'])\n",
    "\n",
    "print(\"You have \", npdata.shape[0], \"training instances\")\n",
    "print(\"You have \", npdata.shape[1], \"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# randomly select a subset of your data (size = 20)\n",
    "testid = [1, 1]\n",
    "while len(testid) != len(set(testid)):\n",
    "    testid = np.random.randint(0, npdata.shape[0], 2000)\n",
    "\n",
    "# Get your testing data\n",
    "print(testid)\n",
    "testset = npdata[testid, :]\n",
    "testtarget = nptarget[testid]\n",
    "print(testset.shape)\n",
    "\n",
    "trainset = npdata\n",
    "traintarget = nptarget\n",
    "\n",
    "# Get your training data\n",
    "#trainset = np.delete(npdata, testid, 0)\n",
    "#traintarget = np.delete(nptarget, testid, 0)\n",
    "print(trainset.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk import FreqDist\n",
    "import glob\n",
    "from nltk.corpus import stopwords\n",
    "import math\n",
    "import re\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn import metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = featureVectors\n",
    "y = df_unique['bool_russian']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MLPClassifier(hidden_layer_sizes=(13,13,13),max_iter=500)\n",
    "mlp.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### NEURAL NET CLASSIFIER                                                                                       \n",
    "#clf = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(5, 2), random_state=1)\n",
    "#clf.fit(training, traininglabel)\n",
    "#predicted = clf.predict(testing)\n",
    "#print(\"Accruracy of Multi-Layer Perceptron\")\n",
    "#print(metrics.classification_report(testinglabel, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
