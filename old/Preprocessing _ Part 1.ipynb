{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of tweets is  203451\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "stemmer = nltk.stem.porter.PorterStemmer()\n",
    "df = pd.read_csv('tweets.csv')\n",
    "df[0:10]\n",
    "print('The number of tweets is ', len(df['text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/pandas/core/generic.py:3110: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self[name] = value\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:28: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of unique stemmed tweets is  173905\n",
      "The number of unique stemmed Russian tweets is  121200\n",
      "The number of unique stemmed normal tweets is  52705\n"
     ]
    }
   ],
   "source": [
    "df.dropna(subset=[\"text\"], inplace=True)\n",
    "\n",
    "retweetedRussianTweets = df.retweeted_status_id.isin(df.tweet_id) & df.retweeted_status_id.notnull()\n",
    "df2 = df.drop(df[retweetedRussianTweets].index)\n",
    "\n",
    "df_unique_stemmed = df2.drop_duplicates('text')\n",
    "\n",
    "r = re.compile(r'RT @\\w*:')\n",
    "retweet_series = df_unique_stemmed.text.str.contains(r)\n",
    "rt_field_series = df_unique_stemmed.retweeted_status_id.notnull()\n",
    "\n",
    "df_unique_stemmed[0:10]\n",
    "# df['text'] = df[\"text\"].apply(lambda x: [stemmer.stem(y) for y in x])\n",
    "url_regex = re.compile(r'https?:\\/\\/(www\\.)?[-a-zA-Z0-9@:%._\\+~#=]{1,256}\\.[a-z]{2,6}\\b([-a-zA-Z0-9@:%_\\+.~#?&//=]*)')\n",
    "# username_regex = re.compile(r'RT @\\w*')\n",
    "username_regex = re.compile(r'@(?=\\w)')\n",
    "hashtag_regex = re.compile(r'#(?=\\w)')\n",
    "\n",
    "df_unique_stemmed.text = (df_unique_stemmed.text.str.replace(r,repl='')\n",
    "    .str.replace(url_regex,repl='')\n",
    "    .str.replace(username_regex, repl='')\n",
    "    .str.replace(hashtag_regex, repl='')\n",
    "    .str.replace('\\n', repl='')\n",
    "    .str.replace('\\r', repl='')       \n",
    "#    .apply(lambda x: ' '.join([stemmer.stem(y) for y in x.split(' ')]))\n",
    "                         )\n",
    "\n",
    "df_unique_stemmed.dropna(subset=[\"text\"], inplace=True)\n",
    "df_unique_stemmed[~(~retweet_series & ~rt_field_series)]['text'].to_csv('real_stemmed.txt',index=False)\n",
    "df_unique_stemmed[~retweet_series & ~rt_field_series]['text'].to_csv('russian_stemmed.txt',index=False)\n",
    "\n",
    "\n",
    "print('The number of unique stemmed tweets is ', len(df_unique_stemmed['text']))\n",
    "print('The number of unique normal tweets is ', len(df_unique_stemmed[~(~retweet_series & ~rt_field_series)]['text']))\n",
    "print('The number of unique Russian tweets is ', len(df_unique_stemmed[~retweet_series & ~rt_field_series]['text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/pandas/core/generic.py:3110: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self[name] = value\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:28: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of unique tweets is  173905\n",
      "The number of unique Russian tweets is  121200\n",
      "The number of unique normal tweets is  52705\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('./tweets.csv')\n",
    "\n",
    "df.dropna(subset=[\"text\"], inplace=True)\n",
    "\n",
    "retweetedRussianTweets = df.retweeted_status_id.isin(df.tweet_id) & df.retweeted_status_id.notnull()\n",
    "df2 = df.drop(df[retweetedRussianTweets].index)\n",
    "\n",
    "df_unique = df2.drop_duplicates('text')\n",
    "\n",
    "r = re.compile(r'RT @\\w*:')\n",
    "retweet_series = df_unique.text.str.contains(r)\n",
    "rt_field_series = df_unique.retweeted_status_id.notnull()\n",
    "\n",
    "df_unique[0:10]\n",
    "# df['text'] = df[\"text\"].apply(lambda x: [stemmer.stem(y) for y in x])\n",
    "url_regex = re.compile(r'https?:\\/\\/(www\\.)?[-a-zA-Z0-9@:%._\\+~#=]{1,256}\\.[a-z]{2,6}\\b([-a-zA-Z0-9@:%_\\+.~#?&//=]*)')\n",
    "# username_regex = re.compile(r'RT @\\w*')\n",
    "username_regex = re.compile(r'@(?=\\w)')\n",
    "hashtag_regex = re.compile(r'#(?=\\w)')\n",
    "\n",
    "df_unique.text = (df_unique.text.str.replace(r,repl='')\n",
    "    .str.replace(url_regex,repl='')\n",
    "    .str.replace(username_regex, repl='')\n",
    "    .str.replace(hashtag_regex, repl='')\n",
    "    .str.replace('\\n', repl='')\n",
    "    .str.replace('\\r', repl=''))\n",
    "\n",
    "df_unique.dropna(subset=[\"text\"], inplace=True)\n",
    "\n",
    "df_unique[~(~retweet_series & ~rt_field_series)]['text'].to_csv('real.txt',index=False)\n",
    "df_unique[~retweet_series & ~rt_field_series]['text'].to_csv('russian.txt',index=False)\n",
    "# df_original[\"russian\"] = 0\n",
    "\n",
    "print('The number of unique unstemmed tweets is ', len(df_unique['text']))\n",
    "print('The number of unique normal tweets is ', len(df_unique[~(~retweet_series & ~rt_field_series)]['text']))\n",
    "print('The number of unique Russian tweets is ', len(df_unique[~retweet_series & ~rt_field_series]['text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>bool_russian</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ThingsDoneByMistake kissing auntie in the lips</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TheOlderWeGet the more pessimistic we are</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ready To Feel Like A Failure? Joan Of Arc Was...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Amen! blacklivesmatter</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Twitchy: Chuck Todd caught out there shilling...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>BernieSanders Trump people should rally TOGET...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>HillaryClinton The undecided voters on that s...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>TodayCleveland 'no way'</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>NickTomaWBRE Hi, Nick! We're holding a \"Miners...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>What. Is. A. Resolution My4WordNewYearsResolution</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  bool_russian\n",
       "0     ThingsDoneByMistake kissing auntie in the lips             1\n",
       "1         TheOlderWeGet the more pessimistic we are              0\n",
       "2   Ready To Feel Like A Failure? Joan Of Arc Was...             0\n",
       "3                            Amen! blacklivesmatter              1\n",
       "4   Twitchy: Chuck Todd caught out there shilling...             0\n",
       "5   BernieSanders Trump people should rally TOGET...             0\n",
       "6   HillaryClinton The undecided voters on that s...             0\n",
       "7                            TodayCleveland 'no way'             1\n",
       "8  NickTomaWBRE Hi, Nick! We're holding a \"Miners...             1\n",
       "9  What. Is. A. Resolution My4WordNewYearsResolution             1"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Add a column bool_russian which is our target column for the classifier\n",
    "df_unique['bool_russian'] = (~retweet_series & ~rt_field_series).apply(lambda x: 1 if x else 0)\n",
    "df_unique[:10][['text','bool_russian']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(8571 unique tokens: ['kissing', 'thingsdonebymistake', '19', 'burned', 'feel']...)\n",
      "[['thingsdonebymistake', 'kissing', 'auntie', 'lips'], ['theolderweget', 'pessimistic'], ['ready', 'feel', 'like', 'failure?', 'joan', 'arc', '19', 'burned', 'stake'], ['amen!', 'blacklivesmatter'], ['twitchy:', 'chuck', 'todd', 'caught', 'shilling', 'hillary', 'clintonthe', 'post', 'busted:', 'adam', 'baldwi...', '#…'], ['berniesanders', 'trump', 'people', 'rally', 'together', 'establishment', '💩-ing', 'choices', 'thefix'], ['hillaryclinton', 'undecided', 'voters', 'stage', 'polled', 'trump', 'won.', 'cnn', 'biased.'], ['todaycleveland', \"'no\", \"way'\"], ['nicktomawbre', 'hi,', 'nick!', \"we're\", 'holding', '\"miners', 'trump\"', 'rally', 'tomorrow.', 'interested', 'covering', 'it,', 'ple…'], ['what.', 'is.', 'a.', 'resolution', 'my4wordnewyearsresolution']]\n"
     ]
    }
   ],
   "source": [
    "#dfLength = len(df_unique['text'])\n",
    "\n",
    "## Here we will create out list of stopwords\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stoplist = stopwords.words('english')\n",
    "PS3StopWords = [\"ever\", \"one\", \"do\",\"does\",\"make\", \"go\", \"us\", \"to\", \"get\", \"about\", \"may\", \"s\", \".\", \",\", \"!\", \"i\", \"I\", '\\\"', \"?\", \";\", \"--\", \"--\", \"would\", \"could\", \"”\", \"Mr.\", \"Miss\", \"Mrs.\", \"don’t\", \"said\", \"can't\", \"didn't\", \"aren't\", \"I'm\", \"you're\", \"they're\", \"'s\"]\n",
    "stoplist.extend(PS3StopWords)\n",
    "personalAdditions = [\"it\", \"i'm\", \"|\", \"–\", \"-\", \"~\", \"you're\", \"thing\", '\"', \"…\", '…\"', \"\"]\n",
    "stoplist.extend(personalAdditions)\n",
    "\n",
    "from gensim import corpora\n",
    "tweets = [[word for word in tweet.lower().split() if word not in stoplist] for tweet in df_unique['text']]\n",
    "dictionary = corpora.Dictionary(tweets)\n",
    "\n",
    "## Here we will remove stop words and words that have a frequency less than 5\n",
    "\n",
    "cutoffnumber = 20\n",
    "\n",
    "from six import iteritems\n",
    "stop_ids = [dictionary.token2id[stopword] for stopword in stoplist\n",
    "           if stopword in dictionary.token2id]\n",
    "once_ids = [tokenid for tokenid, docfreq in iteritems(dictionary.dfs) if docfreq < cutoffnumber]\n",
    "dictionary.filter_tokens(stop_ids + once_ids)  # remove stop words and words that appear only once\n",
    "dictionary.compactify()  # remove gaps in id sequence after words that were removed\n",
    "print(dictionary)\n",
    "print(tweets[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "173905\n",
      "8571\n",
      "[1 1 0 ..., 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "## This following nested for loop creates the raw counts for the feature vectors\n",
    "\n",
    "print(len(df_unique['text'])) # Number of tweets\n",
    "print(len(dictionary)) # Length of the feature vectors\n",
    "\n",
    "featureVectors = np.zeros((len(df_unique['text']),len(dictionary)),int)\n",
    "\n",
    "for i in range(len(df_unique['text'])) :\n",
    "    for word in tweets[i]:\n",
    "        if word in dictionary.token2id :\n",
    "            word_id = dictionary.token2id[word]\n",
    "            featureVectors[i][word_id] += 1\n",
    "            \n",
    "print(featureVectors[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "featureVectors.shape[0]\n",
    "targetvalues = df_unique['bool_russian'].tolist()\n",
    "X_train, X_test, y_train, y_test = train_test_split(featureVectors, targetvalues)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/neural_network/multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (5) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(10, 5, 2), learning_rate='constant',\n",
       "       learning_rate_init=0.001, max_iter=5, momentum=0.9,\n",
       "       nesterovs_momentum=True, power_t=0.5, random_state=None,\n",
       "       shuffle=True, solver='adam', tol=0.0001, validation_fraction=0.1,\n",
       "       verbose=False, warm_start=False)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "layers = (10,5,2)\n",
    "iters = 5\n",
    "mlp = MLPClassifier(hidden_layer_sizes=layers,max_iter = iters)\n",
    "mlp.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5  iterations\n",
      "(10, 5, 2) layers\n",
      "[[27134  3141]\n",
      " [ 5113  8089]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.84      0.90      0.87     30275\n",
      "          1       0.72      0.61      0.66     13202\n",
      "\n",
      "avg / total       0.80      0.81      0.81     43477\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions = mlp.predict(X_test)\n",
    "\n",
    "print(iters, ' iterations')\n",
    "print(layers, 'layers')\n",
    "\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "print(confusion_matrix(y_test,predictions))\n",
    "print(classification_report(y_test,predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "assignClusters(tweets[0:50])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
