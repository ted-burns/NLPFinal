{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"NLP Russian Tweet Classifier TH 130.ipynb","version":"0.3.2","views":{},"default_view":{},"provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"cells":[{"metadata":{"id":"Wz7Jr6oiV4KE","colab_type":"text"},"cell_type":"markdown","source":["### Part 0: Pre-Processing"]},{"metadata":{"id":"fOR2QnmgWUuj","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["!pip install -U -q PyDrive\n","\n","from pydrive.auth import GoogleAuth\n","from pydrive.drive import GoogleDrive\n","from google.colab import auth\n","from oauth2client.client import GoogleCredentials\n","\n","# 1. Authenticate and create the PyDrive client.\n","auth.authenticate_user()\n","gauth = GoogleAuth()\n","gauth.credentials = GoogleCredentials.get_application_default()\n","drive = GoogleDrive(gauth)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"DtN2s_0JZl5H","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0},"base_uri":"https://localhost:8080/","height":34},"outputId":"cd7b6465-b671-46cc-fe63-71e4d6aecf91","executionInfo":{"status":"ok","timestamp":1525979091949,"user_tz":240,"elapsed":395,"user":{"displayName":"Edward Burns","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s128","userId":"100856223951860021590"}}},"cell_type":"code","source":["file_list = drive.ListFile({'q': \"title contains 'NLPFinal_uploadedAt235' and trashed=false\"}).GetList()\n","# file_list = drive.ListFile({})\n","for file1 in file_list:\n","  print('title: %s, id: %s' % (file1['title'], file1['id']))"],"execution_count":14,"outputs":[{"output_type":"stream","text":["title: NLPFinal_uploadedAt235, id: 1E99YL8LvU-mLizrQFR2r3DuLwkUuI1lg\n"],"name":"stdout"}]},{"metadata":{"id":"j6BT4KwpV4KF","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0},"base_uri":"https://localhost:8080/","height":915},"outputId":"169df123-3b75-407b-d3e3-2dd0dc86a5b7","executionInfo":{"status":"error","timestamp":1525977683025,"user_tz":240,"elapsed":298,"user":{"displayName":"Edward Burns","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s128","userId":"100856223951860021590"}}},"cell_type":"code","source":["import pandas as pd \n","import numpy as np\n","import re\n","import nltk\n","stemmer = nltk.stem.porter.PorterStemmer()\n","df = pd.read_csv('../tweets.csv')\n","df[0:10]\n","print('The number of tweets is ', len(df['text']))"],"execution_count":4,"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-4-2f275ff9050e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mstemmer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mporter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPorterStemmer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../tweets.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'The number of tweets is '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, skip_footer, doublequote, delim_whitespace, as_recarray, compact_ints, use_unsigned, low_memory, buffer_lines, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    707\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    708\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 709\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    710\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    711\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    447\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 449\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    450\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    816\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    817\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 818\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    819\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    820\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1047\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1048\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'c'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1049\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1050\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1051\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'python'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1693\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'allow_leading_cols'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_col\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1694\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1695\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1696\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1697\u001b[0m         \u001b[0;31m# XXX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n","\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: File b'../tweets.csv' does not exist"]}]},{"metadata":{"id":"T7MIvsZ0V4KL","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0},"base_uri":"https://localhost:8080/","height":232},"outputId":"e80377f2-8c72-4b13-b6b4-bbd853455987","executionInfo":{"status":"error","timestamp":1525977653327,"user_tz":240,"elapsed":266,"user":{"displayName":"Anthony Trasatti","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s128","userId":"111540680139850476581"}}},"cell_type":"code","source":["df.dropna(subset=[\"text\"], inplace=True)\n","\n","retweetedRussianTweets = df.retweeted_status_id.isin(df.tweet_id) & df.retweeted_status_id.notnull()\n","df2 = df.drop(df[retweetedRussianTweets].index)\n","\n","df_unique_stemmed = df2.drop_duplicates('text')\n","\n","r = re.compile(r'RT @\\w*:')\n","retweet_series = df_unique_stemmed.text.str.contains(r)\n","rt_field_series = df_unique_stemmed.retweeted_status_id.notnull()\n","\n","df_unique_stemmed[0:10]\n","# df['text'] = df[\"text\"].apply(lambda x: [stemmer.stem(y) for y in x])\n","url_regex = re.compile(r'https?:\\/\\/(www\\.)?[-a-zA-Z0-9@:%._\\+~#=]{1,256}\\.[a-z]{2,6}\\b([-a-zA-Z0-9@:%_\\+.~#?&//=]*)')\n","# username_regex = re.compile(r'RT @\\w*')\n","username_regex = re.compile(r'@(?=\\w)')\n","hashtag_regex = re.compile(r'#(?=\\w)')\n","\n","df_unique_stemmed.text = (df_unique_stemmed.text.str.replace(r,repl='')\n","    .str.replace(url_regex,repl='')\n","    .str.replace(username_regex, repl='')\n","    .str.replace(hashtag_regex, repl='')\n","    .str.replace('\\n', repl='')\n","    .str.replace('\\r', repl='')       \n","    .apply(lambda x: ' '.join([stemmer.stem(y) for y in x.split(' ')]))\n","                         )\n","\n","df_unique_stemmed.dropna(subset=[\"text\"], inplace=True)\n","df_unique_stemmed[~(~retweet_series & ~rt_field_series)]['text'].to_csv('real_stemmed.txt',index=False)\n","df_unique_stemmed[~retweet_series & ~rt_field_series]['text'].to_csv('russian_stemmed.txt',index=False)\n","\n","\n","print('The number of unique stemmed tweets is ', len(df_unique_stemmed['text']))\n","print('The number of unique normal tweets is ', len(df_unique_stemmed[~(~retweet_series & ~rt_field_series)]['text']))\n","print('The number of unique Russian tweets is ', len(df_unique_stemmed[~retweet_series & ~rt_field_series]['text']))"],"execution_count":2,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-2-07057226c947>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"text\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mretweetedRussianTweets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretweeted_status_id\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtweet_id\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretweeted_status_id\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnotnull\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdf2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mretweetedRussianTweets\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"]}]},{"metadata":{"id":"w779REmDV4KQ","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}},"outputId":"1a8a00db-e9ad-4ffa-a087-0e40d493e3e7"},"cell_type":"code","source":["df = pd.read_csv('./tweets.csv')\n","\n","df.dropna(subset=[\"text\"], inplace=True)\n","\n","retweetedRussianTweets = df.retweeted_status_id.isin(df.tweet_id) & df.retweeted_status_id.notnull()\n","df2 = df.drop(df[retweetedRussianTweets].index)\n","\n","df_unique = df2.drop_duplicates('text')\n","\n","r = re.compile(r'RT @\\w*:')\n","retweet_series = df_unique.text.str.contains(r)\n","rt_field_series = df_unique.retweeted_status_id.notnull()\n","\n","df_unique[0:10]\n","# df['text'] = df[\"text\"].apply(lambda x: [stemmer.stem(y) for y in x])\n","url_regex = re.compile(r'https?:\\/\\/(www\\.)?[-a-zA-Z0-9@:%._\\+~#=]{1,256}\\.[a-z]{2,6}\\b([-a-zA-Z0-9@:%_\\+.~#?&//=]*)')\n","# username_regex = re.compile(r'RT @\\w*')\n","username_regex = re.compile(r'@(?=\\w)')\n","hashtag_regex = re.compile(r'#(?=\\w)')\n","\n","df_unique.text = (df_unique.text.str.replace(r,repl='')\n","    .str.replace(url_regex,repl='')\n","    .str.replace(username_regex, repl='')\n","    .str.replace(hashtag_regex, repl='')\n","    .str.replace('\\n', repl='')\n","    .str.replace('\\r', repl=''))\n","\n","df_unique.dropna(subset=[\"text\"], inplace=True)\n","\n","df_unique[~(~retweet_series & ~rt_field_series)]['text'].to_csv('real.txt',index=False)\n","df_unique[~retweet_series & ~rt_field_series]['text'].to_csv('russian.txt',index=False)\n","# df_original[\"russian\"] = 0\n","\n","print('The number of unique unstemmed tweets is ', len(df_unique['text']))\n","print('The number of unique normal tweets is ', len(df_unique[~(~retweet_series & ~rt_field_series)]['text']))\n","print('The number of unique Russian tweets is ', len(df_unique[~retweet_series & ~rt_field_series]['text']))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["/anaconda3/lib/python3.6/site-packages/pandas/core/generic.py:3110: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n","  self[name] = value\n","/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:28: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame\n","\n","See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"],"name":"stderr"},{"output_type":"stream","text":["The number of unique unstemmed tweets is  173905\n","The number of unique normal tweets is  121200\n","The number of unique Russian tweets is  52705\n"],"name":"stdout"}]},{"metadata":{"id":"ah3AWOwNV4KT","colab_type":"text"},"cell_type":"markdown","source":["### Part 1: Russian Tweet Classifier"]},{"metadata":{"id":"RFpYJm2PV4KU","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}},"outputId":"286f3cf3-bf57-4e85-a425-ed26ca9c7531"},"cell_type":"code","source":["## Add a column bool_russian which is our target column for the classifier\n","df_unique_stemmed['bool_russian'] = (~retweet_series & ~rt_field_series).apply(lambda x: 1 if x else 0)\n","df_unique_stemmed[:10][['text','bool_russian']]"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style>\n","    .dataframe thead tr:only-child th {\n","        text-align: right;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: left;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>text</th>\n","      <th>bool_russian</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>thingsdonebymistak kiss aunti in the lip</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>theolderweget the more pessimist we are</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>readi To feel like A failure? joan Of arc wa ...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>amen! blacklivesmatt</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>twitchy: chuck todd caught out there shill fo...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>berniesand trump peopl should ralli togeth ag...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>hillaryclinton the undecid voter on that stag...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>todaycleveland 'no way'</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>nicktomawbr hi, nick! we'r hold a \"miner for t...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>what. is. A. resolut my4wordnewyearsresolut</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                                text  bool_russian\n","0           thingsdonebymistak kiss aunti in the lip             1\n","1           theolderweget the more pessimist we are              0\n","2   readi To feel like A failure? joan Of arc wa ...             0\n","3                              amen! blacklivesmatt              1\n","4   twitchy: chuck todd caught out there shill fo...             0\n","5   berniesand trump peopl should ralli togeth ag...             0\n","6   hillaryclinton the undecid voter on that stag...             0\n","7                            todaycleveland 'no way'             1\n","8  nicktomawbr hi, nick! we'r hold a \"miner for t...             1\n","9        what. is. A. resolut my4wordnewyearsresolut             1"]},"metadata":{"tags":[]},"execution_count":4}]},{"metadata":{"id":"60fBi-KXV4KX","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}},"outputId":"4b983425-80cd-4a3f-8317-d893d67c9b32"},"cell_type":"code","source":["#dfLength = len(df_unique['text'])\n","\n","## Here we will create out list of stopwords\n","\n","from nltk.corpus import stopwords\n","\n","stoplist = stopwords.words('english')\n","PS3StopWords = [\"ever\", \"one\", \"do\",\"does\",\"make\", \"go\", \"us\", \"to\", \"get\", \n","                \"about\", \"may\", \"s\", \".\", \",\", \"!\", \"i\", \"I\", '\\\"', \"?\", \";\", \n","                \"--\", \"--\", \"would\", \"could\", \"‚Äù\",  \"don‚Äôt\", \n","                \"said\", \"can't\", \"didn't\", \"aren't\", \"I'm\", \"you're\", \"they're\", \"'s\"]\n","stoplist.extend(PS3StopWords)\n","personalAdditions = [\"it\", \"i'm\", \"|\", \"‚Äì\", \"-\", \"~\", \"you're\", \"thing\", '\"', \"‚Ä¶\", '‚Ä¶\"', \"\"] # As determined by the top 100 most common tokens\n","stoplist.extend(personalAdditions)\n","\n","## Here we use gensim corpora to create our dictionary of words\n","## This allows us to index our words to create our bag of words feature vector \n","\n","from gensim import corpora\n","tweets = [[word for word in tweet.lower().split() if word not in stoplist] for tweet in df_unique_stemmed['text']]\n","dictionary = corpora.Dictionary(tweets)\n","\n","print(tweets[0:10],'\\n')\n","\n","## We remove stop words and words with frequency less than 20\n","\n","cutoffnumber = 20\n","\n","from six import iteritems\n","stop_ids = [dictionary.token2id[stopword] for stopword in stoplist\n","           if stopword in dictionary.token2id]\n","once_ids = [tokenid for tokenid, docfreq in iteritems(dictionary.dfs) if docfreq < cutoffnumber]\n","dictionary.filter_tokens(stop_ids + once_ids)  # remove stop words and words that appear only once\n","dictionary.compactify()  # remove gaps in id sequence after words that were removed\n","print(dictionary)\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["[['thingsdonebymistak', 'kiss', 'aunti', 'lip'], ['theolderweget', 'pessimist'], ['readi', 'feel', 'like', 'failure?', 'joan', 'arc', 'wa', 'onli', '19', 'wa', 'burn', 'stake'], ['amen!', 'blacklivesmatt'], ['twitchy:', 'chuck', 'todd', 'caught', 'shill', 'hillari', 'clintonth', 'post', 'busted:', 'adam', 'baldwi...', '#‚Ä¶'], ['berniesand', 'trump', 'peopl', 'ralli', 'togeth', 'establish', 'üí©-ing', 'choic', 'thefix'], ['hillaryclinton', 'undecid', 'voter', 'stage', 'wa', 'poll', 'trump', 'won.', 'cnn', 'biased.'], ['todaycleveland', \"'no\", \"way'\"], ['nicktomawbr', 'hi,', 'nick!', \"we'r\", 'hold', '\"miner', 'trump\"', 'ralli', 'tomorrow.', \"you'r\", 'interest', 'cover', 'it,', 'ple‚Ä¶'], ['what.', 'is.', 'a.', 'resolut', 'my4wordnewyearsresolut']] \n","\n","Dictionary(7363 unique tokens: ['kiss', 'lip', 'thingsdonebymistak', '19', 'burn']...)\n"],"name":"stdout"}]},{"metadata":{"id":"LNIq0PMVV4Ka","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}},"outputId":"a7017ca0-cf8d-4f93-e162-c5ed3f971f73"},"cell_type":"code","source":["## This following nested for loop creates the raw counts for the feature vectors\n","\n","print(len(df_unique_stemmed['text'])) # Number of tweets\n","print(len(dictionary)) # Length of the feature vectors\n","\n","featureVectors = np.zeros((len(df_unique_stemmed['text']),len(dictionary)),int)\n","\n","for i in range(len(df_unique_stemmed['text'])) :\n","    for word in tweets[i]:\n","        if word in dictionary.token2id :\n","            word_id = dictionary.token2id[word]\n","            featureVectors[i][word_id] += 1\n","            \n","print(featureVectors[0]) ## Example feature vector"],"execution_count":0,"outputs":[{"output_type":"stream","text":["173905\n","7363\n","[1 1 1 ..., 0 0 0]\n"],"name":"stdout"}]},{"metadata":{"id":"5pqIx-aOV4Ke","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["## Here we split our data into training set and test set\n","\n","from sklearn.model_selection import train_test_split\n","\n","featureVectors.shape[0]\n","targetvalues = df_unique_stemmed['bool_russian'].tolist()\n","featureVectors_train, featureVectors_test, target_train, target_test = train_test_split(featureVectors, targetvalues, random_state=0)\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"cQCCDHMiV4Kh","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}},"outputId":"763518f7-808e-4b2e-e717-42b632c48922"},"cell_type":"code","source":["## Here we train our neural net with the training data\n","\n","from sklearn.neural_network import MLPClassifier\n","layers = (10,5,2)\n","iters = 5\n","mlp = MLPClassifier(hidden_layer_sizes=layers,max_iter = iters)\n","mlp.fit(featureVectors_train,target_train)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["/anaconda3/lib/python3.6/site-packages/sklearn/neural_network/multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (5) reached and the optimization hasn't converged yet.\n","  % self.max_iter, ConvergenceWarning)\n"],"name":"stderr"},{"output_type":"execute_result","data":{"text/plain":["MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n","       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n","       hidden_layer_sizes=(10, 5, 2), learning_rate='constant',\n","       learning_rate_init=0.001, max_iter=5, momentum=0.9,\n","       nesterovs_momentum=True, power_t=0.5, random_state=None,\n","       shuffle=True, solver='adam', tol=0.0001, validation_fraction=0.1,\n","       verbose=False, warm_start=False)"]},"metadata":{"tags":[]},"execution_count":8}]},{"metadata":{"id":"9SEAPaymV4Kk","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}},"outputId":"0880222f-6aa9-48d6-aaa7-72f8d74fcb4e"},"cell_type":"code","source":["## Here we test out model and see its results\n","\n","predictions = mlp.predict(featureVectors_test)\n","\n","print(iters, ' iterations')\n","print(layers, 'layers')\n","\n","from sklearn.metrics import classification_report,confusion_matrix\n","print(confusion_matrix(target_test,predictions))\n","print(classification_report(target_test,predictions))\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["5  iterations\n","(10, 5, 2) layers\n","[[26804  3528]\n"," [ 4712  8433]]\n","             precision    recall  f1-score   support\n","\n","          0       0.85      0.88      0.87     30332\n","          1       0.71      0.64      0.67     13145\n","\n","avg / total       0.81      0.81      0.81     43477\n","\n"],"name":"stdout"}]},{"metadata":{"id":"cFF3SqH4V4Ko","colab_type":"text"},"cell_type":"markdown","source":["## Part 2: Topic Modelling"]},{"metadata":{"id":"nlXZYxL4V4Ko","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["import gensim\n","from nltk.corpus import stopwords\n","import numpy as np\n","import scipy as sp\n","import re\n","from sklearn.cluster import KMeans\n","import nltk"],"execution_count":0,"outputs":[]},{"metadata":{"id":"kyTZBokgV4Ks","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}},"outputId":"72c94bef-e0bc-4a6a-f1cd-27085f9f5b69"},"cell_type":"code","source":["## This code will split the texts into training data and test data\n","\n","list_tweetstxt = df_unique_stemmed['text'].tolist()\n","\n","from sklearn.model_selection import train_test_split\n","\n","tweetstxt_train, tweetstxt_test = train_test_split(list_tweetstxt, random_state=0)\n","\n","print(len(tweetstxt_train))\n","print(len(tweetstxt_test))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["130428\n","43477\n"],"name":"stdout"}]},{"metadata":{"id":"4FK9ZFMbV4Kw","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["#### READ IN THE TWEETS\n","\n","tweets = []     # original tweets\n","tweettoks = []  # list of lists of tokens by tweet\n","allTokens = []  # all of the tokens, used to determine most common tokens and thereby enhance the stoplist (see part 1 for implementation of this stoplist)\n"," \n","# Add process tweet text\n","for line in tweetstxt_train:\n","    line = line.rstrip()\n","    tweets.append(line)    \n","    line = re.sub(r\"(^| )[0-9]+($| )\", r\" \", line)  # remove digits\n","    addMe = []\n","    for token in line.split():\n","        lowercaseToken = token.lower()\n","        if lowercaseToken not in stoplist:\n","            allTokens.append(lowercaseToken)\n","            addMe.append(lowercaseToken)\n","    tweettoks.append(addMe)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"X-zEpVKyV4K2","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["def printTweetTokenInfo():\n","    print(\"Length of tweettoks\", len(tweettoks), '\\n')\n","\n","    print(\"Should be same as the number of tweets\")\n","    print(\"Is same?\", len(tweettoks)==len(tweets), '\\n')\n","\n","    print(\"EXAMPLE: First five tweets\", tweets[0:5], '\\n')\n","    \n","    print(\"EXAMPLE: First five tweets' token lists\", tweettoks[0:5], '\\n')\n","    \n","    print(\"Total number of tokens\", len(allTokens), '\\n')\n","    \n","    print(\"EXAMPLE: First 10 tokens (of all tweets)\", allTokens[0:10], '\\n')"],"execution_count":0,"outputs":[]},{"metadata":{"id":"ySei1gtQV4K7","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["#printTweetTokenInfo()  # Sanity check"],"execution_count":0,"outputs":[]},{"metadata":{"id":"wQi9Wp60V4K-","colab_type":"text"},"cell_type":"markdown","source":["## Part 2.2 Identify tweet topics for training data (using tweet token lists created above)\n","\n","\n","Identify topics in the tweets using word2vec word embeddings."]},{"metadata":{"id":"LH7oRw25V4K-","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["#### Create the word2vec model used to make tweet vectors in semantic space\n","\n","bigmodel = gensim.models.KeyedVectors.load_word2vec_format(\"GoogleNews-vectors-negative300-SLIM.bin\", binary=True)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"FFfTNj45V4LA","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["#### Using a word2vec model (taken from GoogleNews), read in the normalized vectors for each token, and sum the vectors to create a single tweet vector. Store that vector for future use / k-means clustering.\n","\n","tweetvectors = []   # this list will contain one 300-dimensional vector per headline\n","\n","for tweetTokens in tweettoks:\n","    totvec = np.zeros(300)\n","    for tok in tweetTokens:\n","        if tok.lower() in bigmodel:\n","            totvec = totvec + bigmodel[tok.lower()]\n","    tweetvectors.append(totvec)\n","\n","#print(len(tweetvectors))      # Check that the number of vectors and tweets are the same\n","#print(len(tweets))            #     this number should be the same as the last\n","#print(len(tweetvectors[10]))  # Check to make sure this is a 300-Dimensional array"],"execution_count":0,"outputs":[]},{"metadata":{"id":"G2b9QrWtV4LC","colab_type":"text"},"cell_type":"markdown","source":["### Uncomment the following cell to build the topic model. \n","\n","This takes about 5 minutes. Otherwise, continue with saved model."]},{"metadata":{"id":"gpvyLeZoV4LD","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["#### Use K-means clustering to sort our testing tweets into num_topics topics\n","\n","#import pickle \n","num_topics = 50\n","\n","#kmtweets = KMeans(n_clusters=num_topics, random_state=0)\n","#tweetclusters = kmtweets.fit_predict(tweetvectors)\n","\n","#filename = 'kmTweetsModel_50Topics.sav'\n","#pickle.dump(kmtweets, open(filename, 'wb'))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"34u0pVU-V4LF","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["import pickle\n","\n","# load the model from disk\n","loaded_kmtweets = pickle.load(open('kmTweetsModel_50Topics.sav', 'rb'))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"WIAaRX_2V4LH","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["##### Print out all the headlines that belong to one of the clusters. (UNCOMMENT CODE TO PRINT)\n","\n","#desired_topic_cluster = 30\n","#\n","#for i in range(len(tweetclusters)):\n","#    if tweetclusters[i] == desired_topic_cluster:\n","#        print(tweets[i])"],"execution_count":0,"outputs":[]},{"metadata":{"id":"aG4ysGEnV4LJ","colab_type":"text"},"cell_type":"markdown","source":["## Method to sort incoming tweets into the nearest cluster\n","\n","This method will be used to sort test tweets into their nearest cluster, which will be included in our classifier as a feature from 1-num_clusters\n","This method takes in an array of tweets, and returns their intended cluster (based upon the above model)\n","\n","(Our training data will use the cluster assigned to it-- contained in the list tweetclusters -- above as a fetaure in the final classifier.)"]},{"metadata":{"id":"LW8wdscoV4LK","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["def assignClusters(tweets):\n","    \n","    ##### Separate each tweet by tokens\n","    \n","    toksByTweet = []\n","\n","    for tweet in tweets:\n","        line = tweet.rstrip()\n","        line = re.sub(r\"(^| )[0-9]+($| )\", r\" \", line)  # remove digits\n","        addMe = [token.lower() for token in line.split() if token.lower() not in stoplist]\n","        toksByTweet.append(addMe)\n","        \n","    ##### Turn each tweet's token list into a normalized (based upon the model, not the training tweets) vector\n","    \n","    vectors = []   # this list will contain one 300-dimensional vector per headline\n","\n","    for toks in toksByTweet:\n","        totvec = np.zeros(300)\n","        for tok in toks:\n","            if tok.lower() in bigmodel:\n","                totvec = totvec + bigmodel[tok.lower()]\n","        vectors.append(totvec)\n","        \n","    \n","    #### Return the predicted topic\n","\n","    return loaded_kmtweets.predict(vectors)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"McyU00SrV4LM","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}},"outputId":"1ec1bca5-43c5-4402-df36-654298e11241"},"cell_type":"code","source":["print(\"Let's check, does it correctly classify the first 50 training tweets?\")        # Uncomment for a sanity check\n","print(assignClusters(tweets[0:50])==tweetclusters[0:50])"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Let's check, does it correctly classify the first 50 training tweets?\n","[ True  True  True  True  True  True  True  True  True  True  True  True\n","  True  True  True  True  True  True  True  True  True  True  True  True\n","  True  True  True  True  True  True  True  True  True  True  True  True\n","  True  True  True  True  True  True  True  True  True  True  True  True\n","  True  True]\n"],"name":"stdout"}]},{"metadata":{"id":"EKGbdKPOV4LP","colab_type":"text"},"cell_type":"markdown","source":["## Part 3: Tweet classifier with tweet topic included as a feature"]},{"metadata":{"id":"IQQ4naCtV4LQ","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["featureVectors.shape[0]\n","featureVectors.shape[1]\n","featureVectors[0].shape[0]\n","\n","new_feature_vectors = np.ndarray((featureVectors.shape[0], featureVectors.shape[1] + num_topics), int)\n","\n","for i in range(featureVectors.shape[0]):\n","    topic_features = np.zeros(num_topics, int)\n","    assignClusters\n","    new_feature_vectors[i] = np.append(featureVectors[i],topic_features)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"_m7l_1PzV4LR","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]}]}