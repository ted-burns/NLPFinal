{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-process training tweets (make token lists by tweet for later use in vector creation)\n",
    "\n",
    "Filter out stopwords, make lowercase, concatenate tweets that are Russian and non-Russian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import re\n",
    "from sklearn.cluster import KMeans\n",
    "import nltk\n",
    "\n",
    "stoplist = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#### READ IN THE TWEETS\n",
    "\n",
    "tweets = []     # original tweets\n",
    "tweettoks = []  # list of lists of tokens by tweet\n",
    "allTokens = []  # all of the tokens in Russian tweets\n",
    " \n",
    "# Real tweets\n",
    "f = open(\"real_stemmed.txt\")\n",
    "for line in f:\n",
    "    line = line.rstrip()\n",
    "    tweets.append(line)    \n",
    "    line = re.sub(r\"(^| )[0-9]+($| )\", r\" \", line)  # remove digits\n",
    "    addMe = []\n",
    "    for token in line.split():\n",
    "        lowercaseToken = token.lower()\n",
    "        if lowercaseToken not in stoplist:\n",
    "            allTokens.append(lowercaseToken)\n",
    "            addMe.append(lowercaseToken)\n",
    "    tweettoks.append(addMe)\n",
    "f.close()\n",
    "\n",
    "# Russian bot tweets\n",
    "f = open(\"russian_stemmed.txt\")\n",
    "for line in f:\n",
    "    line = line.rstrip()\n",
    "    tweets.append(line)    \n",
    "    line = re.sub(r\"(^| )[0-9]+($| )\", r\" \", line)  # remove digits\n",
    "    addMe = []\n",
    "    for token in line.split():\n",
    "        lowercaseToken = token.lower()\n",
    "        if lowercaseToken not in stoplist:\n",
    "            allTokens.append(lowercaseToken)\n",
    "            addMe.append(lowercaseToken)\n",
    "    tweettoks.append(addMe)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def printTweetTokenInfo():\n",
    "    print(\"Length of tweettoks\")\n",
    "    print(len(tweettoks))\n",
    "    print()\n",
    "\n",
    "    print(\"Should be same as the number of tweets\")\n",
    "    print(\"Is same?\")\n",
    "    print(len(tweettoks)==len(tweets))\n",
    "    print()\n",
    "\n",
    "    print(\"EXAMPLE: First five tweets\")\n",
    "    print(tweets[0:5])\n",
    "    print()\n",
    "    \n",
    "    print(\"EXAMPLE: First five tweets' token lists\")\n",
    "    print(tweettoks[0:5])\n",
    "    print()\n",
    "    \n",
    "    print(\"Total number of tokens\")\n",
    "    print(len(allTokens))\n",
    "    print()\n",
    "    \n",
    "    print(\"EXAMPLE: First 10 tokens (of all tweets)\")\n",
    "    print(allTokens[0:10])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#printTweetTokenInfo()  # Sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Remove any stopwords from most common 100 that seem superfluous (in addition to stopwords compiled from PS3)\n",
    "\n",
    "# Execute the commented out code to see the most common 100 tokens that we used to select additional stopwords\n",
    "#fdist = nltk.FreqDist(allTokens)\n",
    "#print(fdist.most_common(100))\n",
    "\n",
    "PS3StopWords = [\"ever\", \"one\", \"do\",\"does\",\"make\", \"go\", \"us\", \"to\", \"get\", \"about\", \"may\", \"s\", \".\", \",\", \"!\", \"i\", \"I\", '\\\"', \"?\", \";\", \"--\", \"--\", \"would\", \"could\", \"”\", \"Mr.\", \"Miss\", \"Mrs.\", \"don’t\", \"said\", \"can't\", \"didn't\", \"aren't\", \"I'm\", \"you're\", \"they're\", \"'s\"]\n",
    "stoplist.extend(PS3StopWords)\n",
    "personalAdditions = [\"it\", \"i'm\", \"|\", \"–\", \"-\", \"~\", \"you'r\", \"thing\", '\"', \"…\", '…\"', \"\"]\n",
    "stoplist.extend(personalAdditions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## RECALCULATE THE TOKEN LISTS WITHOUT NEW STOPWORDS\n",
    "\n",
    "tweettoks = []  # list of lists of tokens by tweet, EMPTY\n",
    "\n",
    "# Real tweets\n",
    "f = open(\"real_stemmed.txt\")\n",
    "for line in f:\n",
    "    line = line.rstrip()\n",
    "    line = re.sub(r\"(^| )[0-9]+($| )\", r\" \", line)  # remove digits\n",
    "    addMe = [token.lower() for token in line.split() if token.lower() not in stoplist]\n",
    "    tweettoks.append(addMe)\n",
    "f.close()\n",
    "\n",
    "# Russian bot tweets\n",
    "f = open(\"russian_stemmed.txt\")\n",
    "for line in f:\n",
    "    line = line.rstrip()\n",
    "    line = re.sub(r\"(^| )[0-9]+($| )\", r\" \", line)  # remove digits\n",
    "    addMe = [token.lower() for token in line.split() if token.lower() not in stoplist]\n",
    "    tweettoks.append(addMe)\n",
    "f.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#printTweetTokenInfo()  # Sanity check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identify tweet topics for training data (using tweet token lists created above)\n",
    "\n",
    "\n",
    "Identify topics in the tweets using word2vec word embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#### Create the word2vec model used to make tweet vectors in semantic space\n",
    "\n",
    "bigmodel = gensim.models.KeyedVectors.load_word2vec_format(\"GoogleNews-vectors-negative300-SLIM.bin\", binary=True)\n",
    "\n",
    "#### OPTIONAL: Read in an array of new sentences to additionally train the model on those\n",
    "####            this is in the case of obtaining a corpus of tweet / slang language that does not overlap with training data\n",
    "#model.build_vocab(new_sentences, update=True)\n",
    "#model.train(new_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#### Using a word2vec model (taken from GoogleNews), read in the normalized vectors for each token, and sum the vectors to create a single tweet vector. Store that vector for future use / k-means clustering.\n",
    "\n",
    "tweetvectors = []   # this list will contain one 300-dimensional vector per headline\n",
    "\n",
    "for tweetTokens in tweettoks:\n",
    "    totvec = np.zeros(300)\n",
    "    for tok in tweetTokens:\n",
    "        if tok.lower() in bigmodel:\n",
    "            totvec = totvec + bigmodel[tok.lower()]\n",
    "    tweetvectors.append(totvec)\n",
    "\n",
    "#print(len(tweetvectors))      # Check that the number of vectors and tweets are the same\n",
    "#print(len(tweets))            #     this number should be the same as the last\n",
    "#print(len(tweetvectors[10]))  # Check to make sure this is a 300-Dimensional array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#### Use K-means clustering to sort our testing tweets into num_topics topics\n",
    "num_topics = 50\n",
    "\n",
    "kmtweets = KMeans(n_clusters=num_topics, random_state=0)\n",
    "tweetclusters = kmtweets.fit_predict(tweetvectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'kmtweets' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-24eca72be77c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'kmTweetsModel.sav'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkmtweets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# some time later...\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'kmtweets' is not defined"
     ]
    }
   ],
   "source": [
    "import pickle \n",
    "\n",
    "filename = 'kmTweetsModel.sav'\n",
    "pickle.dump(kmtweets, open(filename, 'wb'))\n",
    " \n",
    "# some time later...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# load the model from disk\n",
    "loaded_kmTweetsModel = pickle.load(open('kmTweetsModel.sav', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##### Print out all the headlines that belong to one of the clusters. (UNCOMMENT CODE TO PRINT)\n",
    "\n",
    "#desired_topic_cluster = 30\n",
    "#\n",
    "#for i in range(len(tweetclusters)):\n",
    "#    if tweetclusters[i] == desired_topic_cluster:\n",
    "#        print(tweets[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Method to sort incoming tweets into the nearest cluster\n",
    "\n",
    "This method will be used to sort test tweets into their nearest cluster, which will be included in our classifier as a feature from 1-num_clusters\n",
    "This method takes in an array of tweets, and returns their intended cluster (based upon the above model)\n",
    "\n",
    "(Our training data will use the cluster assigned to it-- contained in the list tweetclusters -- above as a fetaure in the final classifier.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def assignClusters(tweets):\n",
    "    \n",
    "    ##### Separate each tweet by tokens\n",
    "    \n",
    "    toksByTweet = []\n",
    "\n",
    "    for tweet in tweets:\n",
    "        line = tweet.rstrip()\n",
    "        line = re.sub(r\"(^| )[0-9]+($| )\", r\" \", line)  # remove digits\n",
    "        addMe = [token.lower() for token in line.split() if token.lower() not in stoplist]\n",
    "        toksByTweet.append(addMe)\n",
    "        \n",
    "    ##### Turn each tweet's token list into a normalized (based upon the model, not the training tweets) vector\n",
    "    \n",
    "    vectors = []   # this list will contain one 300-dimensional vector per headline\n",
    "\n",
    "    for toks in toksByTweet:\n",
    "        totvec = np.zeros(300)\n",
    "        for tok in toks:\n",
    "            if tok.lower() in bigmodel:\n",
    "                totvec = totvec + bigmodel[tok.lower()]\n",
    "        vectors.append(totvec)\n",
    "        \n",
    "    \n",
    "    #### Return the predicted topic\n",
    "\n",
    "    return loaded_kmTweetsModel.predict(vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let's check, does it correctly classify the first 50 training tweets?\n",
      "[ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True]\n"
     ]
    }
   ],
   "source": [
    "print(\"Let's check, does it correctly classify the first 50 training tweets?\")        # Uncomment for a sanity check\n",
    "print(assignClusters(tweets[0:50])==tweetclusters[0:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
